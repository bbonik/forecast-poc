{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Related Time-Series Data\n",
    "\n",
    "> *This notebook should work well in the `Python 3 (Data Science)` kernel in SageMaker Studio, or `conda_python3` in SageMaker Notebook Instances*\n",
    "\n",
    "In [Notebook 1](1.%20Preparing%20Target%20Time-Series%20Data.ipynb), we prepared training and validation datasets for our **target time-series**: the quantity we're actually trying to predict, which is the minimum required data to get started with Amazon Forecast.\n",
    "\n",
    "In most real-world use cases, forecasts can be significantly improved using:\n",
    "\n",
    "- **Related Time-Series**: Other time-varying factors which can be informative *inputs* to our forecast\n",
    "- **Item Metadata**: Static attributes which help us find correlations between different `item_ids` in our forecast\n",
    "\n",
    "**In this notebook:** we'll use Python code to prepare a Related Time-Series file for our example use case - ignoring item metadata since our example contains only a single `item_id`.\n",
    "\n",
    "## Sourcing Data\n",
    "\n",
    "As we saw right at the start of the first notebook, our raw example traffic data actually **already includes** some time-varying attributes which might make good RTS candidates: **weather information**. This is the data source we'll use in this notebook.\n",
    "\n",
    "In real-world use-cases, extra information might be stored in the same system as your target variable or might be somewhere else: For example bringing weather or product discontinuation/stock data together with demand recorded in sales data.\n",
    "\n",
    "Our goal should be to find variables which are likely *significant and useful to our forecast*. For example:\n",
    "\n",
    "- Adding out-of-stock data to a retail forecast might be very important, because without it we could be training our model to forecast 0 sales on days when actually there is lots of demand - just because we sold out of a product in the past.\n",
    "- Adding a calendar of promotional events in Indonesia might *not* be very relevant, if our forecast is only modelling sales in the Philippines.\n",
    "\n",
    "Since our traffic data is already loaded on the notebook, we'll simply start by loading our required libraries as there's no extra downloading to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "from time import sleep\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Local Dependencies:\n",
    "import util\n",
    "\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our RTS source data was already downloaded in notebook 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing and Pre-Processing the Data\n",
    "\n",
    "As with the TTS, it's vitally important that we understand whether any data might be [missing](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-missing-values.html) from our RTS or if there might be any [conflicts in data frequency](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-datasets-groups.html#howitworks-data-alignment) and how those should be resolved.\n",
    "\n",
    "Although Amazon Forecast provides functionality for handling both these cases (see the linked docs), we need to check that any such treatments are correct for our use case.\n",
    "\n",
    "Because related time-series are **inputs** to our forecast, they must typically span **the forecast horizon as well as the history**, as shown below:\n",
    "\n",
    "![Graph: RTS must extend past TTS into the Forecast Horizon](https://docs.aws.amazon.com/forecast/latest/dg/images/short-long-rts.png)\n",
    "\n",
    "Per [the documentation](https://docs.aws.amazon.com/forecast/latest/dg/related-time-series-datasets.html#related-time-series-historical-futurelooking), **only the CNN-QR algorithm** is able to use so-called \"historical related time-series\" where future values are not provided... and even for this algorithm, RTS data will be much more useful where forecasts are available.\n",
    "\n",
    "Since we'll be exploring a range of algorithms, our goal in preparing this example data will be to **remove any NaN/missing** entries from our weather data.\n",
    "\n",
    "To get started, we'll load up our target dataframe from the first notebook and explore the available data range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_time_series_df = target_df.copy()\n",
    "related_time_series_df = full_df.join(related_time_series_df, how=\"outer\")\n",
    "cols = related_time_series_df.columns.tolist()\n",
    "related_time_series_df = related_time_series_df.loc[\"2017-01-01\":]\n",
    "print(related_time_series_df.index.min())\n",
    "print(related_time_series_df.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that the data covers the range of our target time series of 2017's entire year to the end of our known data about September 2018.\n",
    "\n",
    "However, we will see that many records are missing weather data fields:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_time_series_df[related_time_series_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Remember** as with TTS, These may not be the only values your Forecast Predictor sees as \"missing\", if for example there are hours in the history or forecast period with no record at all!\n",
    "\n",
    "For this example, we will:\n",
    "\n",
    "- Forward-fill these missing values before preparing the data file for Forecast\n",
    "- Assume (correctly) that every required timestep does have **at least one** record in the file (so filling missing cells is sufficient)\n",
    "- Assume (correctly) that every hour timestep has **exactly one** record in the file (so no duplicate or close-together records will get aggregated)\n",
    "- Therefore ignore Forecast's missing value and aggregation configurations, because our input data is fully sanitized.\n",
    "\n",
    "Below we fill the missing values and check again for any issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward-fill missing values:\n",
    "related_time_series_df[cols] = related_time_series_df[cols].replace(\"\", np.nan).ffill()\n",
    "\n",
    "# Re-check for missing:\n",
    "related_time_series_df[related_time_series_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all our missing/duplicate worries solved, let's **review the columns and decide what we should keep:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_time_series_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note here:\n",
    "\n",
    "- `holiday` information is not needed because the use case is in the a supported country, so we can simply use the [Holidays feature within Amazon Forecast](https://docs.aws.amazon.com/forecast/latest/dg/API_SupplementaryFeature.html).\n",
    "- `traffic_volume` is our actual target field, so of course will not be part of the RTS\n",
    "- We'll still need to add the `item_id` field back in to the dataset, as we did for the TTS.\n",
    "- `weather_main` seems pretty redundant if `weather_description` is provided, but the rest of the fields seem interesting.\n",
    "\n",
    "Therefore we'll scope our RTS dataset to the following fields:\n",
    "\n",
    "* `timestamp` - The Index\n",
    "* `temp` - float\n",
    "* `rain_1h` - float\n",
    "* `snow_1h` - float\n",
    "* `clouds_all` - float\n",
    "* `weather_description` - string\n",
    "* `item_id` - string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the columns to keep\n",
    "related_time_series_df = related_time_series_df[[\"temp\", \"rain_1h\", \"snow_1h\", \"clouds_all\"]]\n",
    "\n",
    "# Add in item_id\n",
    "related_time_series_df[\"item_id\"] = \"all\"\n",
    "\n",
    "# Validate the structure\n",
    "related_time_series_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Related Time-Series File\n",
    "\n",
    "Since our uploaded RTS data should extend out into the forecast horizon, there's no need to split it into a training and validation set as we did for the TTS.\n",
    "\n",
    "We'll simply save the full set to a CSV file, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it off as a file:\n",
    "related_time_series_filename = \"related_time_series.csv\"\n",
    "related_time_series_path = f\"{data_dir}/{related_time_series_filename}\"\n",
    "related_time_series_df.to_csv(related_time_series_path, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Data to Amazon S3\n",
    "\n",
    "As before, our final step is to upload the prepared file to Amazon S3 ready for import to Amazon Forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the below with e.g. region = \"ap-southeast-1\" if you didn't run notebook 0\n",
    "%store -r region  \n",
    "assert isinstance(region, str), \"`region` must be a region name string e.g. 'us-east-1'\"\n",
    "\n",
    "# Replace the below e.g. bucket_name = \"DOC-EXAMPLE-BUCKET\" if you didn't run notebook 0\n",
    "%store -r bucket_name \n",
    "assert isinstance(bucket_name, str), \"`bucket_name` must be a data bucket name string\"\n",
    "\n",
    "session = boto3.Session(region_name=region)\n",
    "s3 = session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Related File\n",
    "s3.Bucket(bucket_name).Object(related_time_series_filename).upload_file(related_time_series_path)\n",
    "related_s3uri = f\"s3://{bucket_name}/{related_time_series_filename}\"\n",
    "%store related_s3uri\n",
    "print(f\"Uploaded RTS to {related_s3uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Done!\n",
    "\n",
    "Now our Related Time-Series data is prepared and staged in an Amazon S3 bucket ready to import.\n",
    "\n",
    "In the next notebooks, we'll show how to import this additional dataset and use it to build improved forecast models.\n",
    "\n",
    "You can follow along with either the [notebook 4a (AWS Console)](4a.%20Incorporating%20RTS%20Data%20(Console).ipynb) or [notebook 4b (Python SDK)](4b.%20Incorporating%20RTS%20Data%20(Python%20SDK).ipynb) variant."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
