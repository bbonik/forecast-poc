{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Amazon Forecast Quick-Start\n",
    "\n",
    "> *This notebook should work well in the `Python 3 (Data Science)` kernel in SageMaker Studio, or `conda_python3` in SageMaker Notebook Instances*\n",
    "\n",
    "This notebook presents an initial exploration of Amazon Forecast, focussing on creating initial experiments through the AWS Console.\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "The following assumes:\n",
    "\n",
    "- You've already deployed the [Improving Forecast Accuracy with Machine Learning Solution](https://aws.amazon.com/solutions/implementations/improving-forecast-accuracy-with-machine-learning/) from AWS Solutions, including the default New York City taxi demo dataset.\n",
    "- The SageMaker Studio user (or notebook instance) you're running this notebook on has read access to Amazon CloudFormation and your solution's Forecast data bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and imports\n",
    "\n",
    "> ℹ️ The below `pip install` upgrades are required at the time of writing for running the notebook in SageMaker Studio, to avoid bugs in some specific features including reading pandas DataFrames from Amazon S3, and producing DataFrame summaries.\n",
    "\n",
    "You need to run this cell first. If you modify any installs after `import`ing affected libraries, you may see unexpected errors and need to restart your notebook kernel to ensure a self-consistent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U numpy pandas matplotlib \"s3fs>=2022.01.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the required upgrades complete, we're ready to import the libraries used by this notebook and create clients for the various AWS Services to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "from io import BytesIO\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # The AWS SDK for Python\n",
    "import pandas as pd  # DataFrame (tabular data) utilities\n",
    "\n",
    "# Local Dependencies:\n",
    "import util  # Local helper utilities (code in the util/ folder)\n",
    "\n",
    "\n",
    "# Initialize connectors for AWS services:\n",
    "forecast = boto3.client(\"forecast\")\n",
    "forecastquery = boto3.client(\"forecastquery\")\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading prepared data to Amazon Forecast\n",
    "\n",
    "Careful, well-engineered data curation and preparation is probably the **most important step** you can take to build highly accurate forecasts... So it's important to dive deep on this topic to prepare you for a successful PoC.\n",
    "\n",
    "**However**, training models and producing results takes time.\n",
    "\n",
    "So first, let's walk through the process of actually loading data into Amazon Forecast - and revisit data engineering later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the task\n",
    "\n",
    "In this example, we'll use the New York City Taxi trip record dataset (originally sourced from [nyc.gov](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and also available hosted by AWS [on the AWS Open Data Registry](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)).\n",
    "\n",
    "The forecasting goal will be to predict the **number of pickups in the next 7 days, per hour and per pickup zone** - using yellow taxi data from 2018-12 through 2020-02, to avoid COVID effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating the pre-prepared data\n",
    "\n",
    "If you've deployed the [Improving Forecast Accuracy with Machine Learning Solution](https://aws.amazon.com/solutions/implementations/improving-forecast-accuracy-with-machine-learning/) in your account with demo stack (NYC Taxi Data Downloader) enabled, your account will already contain a snapshot of this data prepared and feature engineered for Amazon Forecast.\n",
    "\n",
    "This will include:\n",
    "\n",
    "- The **Target Time Series (TTS)**: The historical data for the actual quantity you want to forecast (number of pickups, by hour and zone)\n",
    "- The **Item Metadata**: Static metadata for each \"item\" in the dataset (in this case, pickup zones)\n",
    "- The **Related Time Series (RTS)**: Time-varying input variables to improve the forecast (in this case, just an extra time featurization)\n",
    "\n",
    "The code cell below will find these datasets in your environment, checking the files exist on [Amazon S3](https://s3.console.aws.amazon.com/s3/home):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = util.config.find_solution_data_bucket()\n",
    "\n",
    "tts_s3uri = f\"s3://{bucket_name}/train/nyctaxi_weather_auto.csv\"\n",
    "rts_s3uri = f\"s3://{bucket_name}/train/nyctaxi_weather_auto.related.csv\"\n",
    "metadata_s3uri = f\"s3://{bucket_name}/train/nyctaxi_weather_auto.metadata.csv\"\n",
    "\n",
    "\n",
    "# Check these configured objects exist in S3:\n",
    "for uri in [tts_s3uri, metadata_s3uri, rts_s3uri]:\n",
    "    if uri:\n",
    "        try:\n",
    "            s3.Object(bucket_name, tts_s3uri[len(\"s3://\"):].partition(\"/\")[2]).load()\n",
    "            print(f\"Found: {uri}\")\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                raise ValueError(f\"{uri} does not exist in S3!\") from e\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset group\n",
    "\n",
    "The first step when using Amazon Forecast is to create a [dataset group (DSG)](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-datasets-groups.html#howitworks-datasetgroup), which is like a container for your project.\n",
    "\n",
    "Each DSG has **one 'slot' per dataset type**: TTS, RTS, and Metadata. So although you can create multiple predictors (models) and forecasts within a DSG, any parallel experiments that compare different data schemas or data extracts will need to run in separate DSGs.\n",
    "\n",
    "▶️ **Open** the [Amazon Forecast Console](https://console.aws.amazon.com/forecast/home)\n",
    "  - Check you're in the right **AWS Region** for your experiment (top right)\n",
    "  - If necessary, go to \"Dataset Groups\" in collapsible the left sidebar\n",
    "  - You should probably see one pre-existing dataset group created for you by the deployed solution - as shown below:\n",
    "\n",
    "![](static/imgs/NYC/01-Dataset-Groups.png \"Screenshot of Amazon Forecast console showing one pre-created Dataset Group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Create** a **new dataset group** by clicking on the orange button as shown above. Select:\n",
    "  - **Name:** `nyctaxi_manual`\n",
    "  - **Domain:** `Custom`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and import the Target Time Series (TTS)\n",
    "\n",
    "Because the TTS is the only *mandatory* dataset to use Amazon Forecast, you should be automatically directed to define it after creating your Dataset Group.\n",
    "\n",
    "So what is our example dataset's schema? Let's take a look by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_df = pd.read_csv(\n",
    "    tts_s3uri,\n",
    "    header=None,\n",
    "    names=[\"timestamp\", \"item_id\", \"geolocation\", \"target_value\"]\n",
    ")\n",
    "tts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Complete** the \"dataset details\" to define your Target Time Series in line with the data:\n",
    "\n",
    "- **Name:** `nyctaxi_manual_tts`\n",
    "- **Frequency:** `1 hour`\n",
    "- **Schema:** ⚠️ Note that this must match the data *including column order!*\n",
    "    1. Field `timestamp`, of type `timestamp` (with format including the time HH:mm:ss)\n",
    "    2. Field `item_id`, of type `string`\n",
    "    3. Field `geolocation`, of type `geolocation` (Lat/Long decimal degrees)\n",
    "    4. Field `target_value`, of type `float`\n",
    "\n",
    "![](static/imgs/NYC/02-TTS-Schema.png \"Screenshot of Amazon Forecast Console TTS data setup showing entered schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you should automatically be prompted to import the data\n",
    "\n",
    "▶️ **Set up** your dataset import as follows, and then click \"Start\" to kick it off.\n",
    "\n",
    "- **Name:** `nyctaxi_manual_tts_1` (doesn't matter too much, so long as it's unique)\n",
    "- **Timezone:** Set time zone `America/New_York` (although for this dataset, sync with geolocation should be equivalent)\n",
    "- **Data Location:** The `s3://...` URI of the TTS, as shown above\n",
    "- **IAM Role:** For simplicity, you can `Create a new role` and grant access to `Any S3 bucket`\n",
    "\n",
    "![](static/imgs/NYC/03-Create-TTS-Import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and import Item Metadata\n",
    "\n",
    "While it's possible to start training models (predictors) with only a target time-series, adding additional *relevant* data of other types can help improve your forecast.\n",
    "\n",
    "So next, we'll import the other optional dataset types starting with **item metadata**\n",
    "\n",
    "This dataset enables you to define **static** metadata for each \"item\": Whatever that means for your use case (such as product SKUs for retail demand forecasting, or pickup zones in this example case).\n",
    "\n",
    "This metadata helps Amazon Forecast understand **connections** between item IDs in your dataset, to improve performance when using algorithms like DeepAR+ or CNN-QR which can learn global (cross-item) patterns.\n",
    "\n",
    "As shown below the example metadata includes the city borough where each pickup zone is located, and a category for each zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(\n",
    "    metadata_s3uri,\n",
    "    header=None,\n",
    "    names=[\"item_id\", \"pickup_borough\", \"binned_max_item\"]\n",
    ")\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Open** your newly created `nyctaxi_manual` DSG and select **Datasets** from the left sidebar to view the details of available dataset slots:\n",
    "\n",
    "![](static/imgs/NYC/04-Datasets-TTSOnly.png \"Screenshot of DSG 'datasets' page showing unused metadata and RTS slots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Click** the \"Upload dataset\" button and select `ITEM_METADATA`\n",
    "\n",
    "▶️ **Configure** your dataset details as below:\n",
    "\n",
    "- **Name**: `nyctaxi_manual_metadata`\n",
    "- **Schema**: (With order matching the dataset above, remember!)\n",
    "  1. Field `item_id` of type `string`\n",
    "  2. Field `pickup_borough` of type `string`\n",
    "  3. Field `binned_max_item` of type `string`\n",
    "\n",
    "![](static/imgs/NYC/05-Metadata-Schema.png \"Screenshot of Item Metadata dataset schema definition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Set up** your dataset import as follows, and then click \"Start\" to kick it off.\n",
    "\n",
    "- **Name**: `nyctaxi_manual_metadata_1`\n",
    "- **Data Location**: The s3://... URI of the **Item Metadata**, as shown earlier\n",
    "- **IAM Role**: Use the same role you created earlier (should show in drop-down)\n",
    "\n",
    "![](static/imgs/NYC/06-Create-Metadata-Import.png \"Screenshot of Item Metadata import job settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and import Related Time Series (RTS)\n",
    "\n",
    "The **Related Time Series** is where we can define **time-varying** input variables for the forecast: For example describing item prices and discounts in retail, or custom featurizations of factors such as economic environment, competitor pressure indicators, or public health concerns.\n",
    "\n",
    "Although some algorithms in Amazon Forecast are able to get benefit from [historical-only Related Time Series](https://docs.aws.amazon.com/forecast/latest/dg/related-time-series-datasets.html#related-time-series-historical-futurelooking), RTS are generally **much more valuable when we also provide them throughout the future forecast window**.\n",
    "\n",
    "This means the end date of your RTS dataset will usually be later than your TTS: since it will include known or predicted inputs over the period you're trying to forecast.\n",
    "\n",
    "One other idea you might consider for a Related Time Series is *weather* data... But in this example, we'll use the already-provided [Amazon Forecast Weather Index](https://docs.aws.amazon.com/forecast/latest/dg/weather.html).\n",
    "\n",
    "So for the example dataset, our RTS includes just one field - a custom time-of-day featurization, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_df = pd.read_csv(\n",
    "    rts_s3uri,\n",
    "    header=None,\n",
    "    names=[\"timestamp\", \"item_id\", \"geolocation\", \"day_hour_name\"]\n",
    ")\n",
    "rts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Click** the \"Upload dataset\" button from the *Datasets* screen as we showed before, and this time select `RELATED_TIME_SERIES` (if prompted)\n",
    "\n",
    "▶️ **Configure** your dataset details as below:\n",
    "\n",
    "- **Name**: `nyctaxi_manual_rts`\n",
    "- **Frequency**: `1 hour`\n",
    "- **Fields:** (With order matching the dataset above, remember!)\n",
    "    1. Field `timestamp` of type `timestamp` (including HH:mm:ss)\n",
    "    2. Field `item_id` of type `string`\n",
    "    3. Field `geolocation` of type `geolocation` (Lat/Long decimal)\n",
    "    4. Field `day_hour_name` of type `string`\n",
    "\n",
    "![](static/imgs/NYC/07-RTS-Schema.png \"Screenshot of Related Time Series schema definition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Set up** your dataset import as follows, and then click \"Start\" to kick it off.\n",
    "\n",
    "- **Name**: `nyctaxi_manual_rts_1`\n",
    "- **Data Location**: The s3://... URI of the **RTS**, as shown earlier\n",
    "- **IAM Role**: Use the same role you created earlier (should show in drop-down)\n",
    "\n",
    "![](static/imgs/NYC/08-Create-RTS-Import.png \"Screenshot of Related Time Series import job settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for all import jobs to complete\n",
    "\n",
    "After defining schemas and starting import jobs for all 3 dataset types (TTS, Metadata, and RTS) - you'll be able to check on import status from the **Datasets** list within your Dataset Group:\n",
    "\n",
    "![](static/imgs/NYC/09-Waiting-For-RTS-Import.png \"Screenshot of datasets showing RTS not yet finished importing\")\n",
    "\n",
    "▶️ **Wait** for all 3 datasets to show as `Active` **AND** for their \"Last import status\" to be `Active` also, before moving on to start training predictors.\n",
    "\n",
    "- ⏰ Dataset imports may take several minutes to complete, and do not scale linearly with dataset size (because of overheads in creating the jobs, and the scalability of underlying infrastructure for large jobs)\n",
    "\n",
    "> ℹ️ **Note:** You can click on the name hyperlink of your datasets to view more details: Including summary statistics, the history of import jobs, and also an ability to **export** the data to S3 again:\n",
    "\n",
    "![](static/imgs/NYC/10-RTS-Dataset-Details.png \"Screenshot of RTS dataset details page showing statistics and export option\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Forecast models ('Predictors')\n",
    "\n",
    "Once your datasets are defined and imported, you're ready to start training forecasting models.\n",
    "\n",
    "▶️ If the `Predictors` item in the left sidebar is available, you can select that and click the orange \"Train new predictor\" button to get started. Otherwise, go to your DSG's `Dashboard` and click **Start predictor training**\n",
    "\n",
    "![](static/imgs/NYC/11-Dashboard-Start-Predictors.png \"DSG Dashboard screenshot showing 'start' button for predictor training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an AutoPredictor\n",
    "\n",
    "Amazon Forecast [AutoPredictors](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-predictor.html) automatically train multiple [forecasting algorithms](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-choosing-recipes.html) on your dataset and produce an **ensemble model** combining these algorithms to deliver the most accurate results for each item in your forecast.\n",
    "\n",
    "▶️ **Configure** your Predictor as below and then click 'Create'\n",
    "\n",
    "- **Name**: `nyctaxi_manual_autopredictor`\n",
    "- **Forecast Frequency**: `1 hour`\n",
    "- **Forecast Horizon**: `168` (7 days at 24 hours per day)\n",
    "- **Forecast Dimensions**: Add `geolocation`\n",
    "- **Forecast Quantiles**: Leave as default `0.1, 0.5, 0.9`\n",
    "- **AutoPredictor** `Enabled`\n",
    "- **Optimization Metric**: Blank or `MAPE`\n",
    "- **Explainability**: `Enabled`\n",
    "- **Weather Index**: `Enabled`\n",
    "- **Holidays**: `Enabled` (United States)\n",
    "\n",
    "![](static/imgs/NYC/12-AutoPredictor-Part-1.png \"Screenshot of AutoPredictor configuration - first section\")\n",
    "\n",
    "![](static/imgs/NYC/13-AutoPredictor-Part-2.png \"Screenshot of AutoPredictor configuration - second section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Legacy Predictor (Prophet Algorithm)\n",
    "\n",
    "Amazon Forecast [Legacy Predictors](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-predictor.html#legacy-predictors) select a single algorithm for prediction, rather than producing an ensemble model: Either manually selected or found via an AutoML process somewhat similar to AutoPredictor.\n",
    "\n",
    "While AutoPredictors generally deliver [higher accuracy and also support explainability](https://aws.amazon.com/blogs/machine-learning/new-amazon-forecast-api-that-creates-up-to-40-more-accurate-forecasts-and-provides-explainability/), Legacy Predictors are of interest to us here because being able to manually specify an algorithm will help us demonstrate a (less good) trained model **faster**.\n",
    "\n",
    "> ℹ️ **Note:** AutoPredictors and Legacy Predictors also carry different [pricing](https://aws.amazon.com/forecast/pricing/), reflecting their different resource utilizations.\n",
    "\n",
    "For this example, we'll train a Legacy Predictor with the statistical, non-deep-learning [Prophet algorithm](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-prophet.html) to demonstrate some quick results.\n",
    "\n",
    "▶️ **Click** 'Train Predictor' again from your DSG Dashboard, or go to 'Datasets' from the left sidebar and click 'Train new predictor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Configure** your Predictor as below and then click 'Create'\n",
    "\n",
    "- **Name**: `nyctaxi_manual_prophet`\n",
    "- **Forecast Frequency**: `1 hour`\n",
    "- **Forecast Horizon**: `168` (7 days at 24 hours per day)\n",
    "- **Forecast Dimensions**: Add `geolocation`\n",
    "- **Forecast Quantiles**: Leave as default `0.1, 0.5, 0.9`\n",
    "- **AutoPredictor** `DISABLED`\n",
    "- **Optimization Metric**: Blank or `MAPE`\n",
    "- **Number of Backtest Windows:** `3`\n",
    "- **Backtest Window Offset:** `168` (Same as Forecast Horizon)\n",
    "- **Weather Index**: `Enabled`\n",
    "- **Holidays**: `Enabled` (United States)\n",
    "\n",
    "> ⚠️ **NOTE: If you no longer see the option to disable AutoPredictor**\n",
    ">\n",
    "> If you don't see any UI option to disable AutoPredictor and create a legacy predictor, you'll need to perform this step through the APIs instead. You can run the code cell after these screenshots to create your predictor programmatically.\n",
    "\n",
    "![](static/imgs/NYC/14-Prophet-Part-1.png \"Screenshot of Prophet predictor configuration - first section\")\n",
    "\n",
    "![](static/imgs/NYC/15-Prophet-Part-2.png \"Screenshot of Prophet predictor configuration - second section\")\n",
    "\n",
    "![](static/imgs/NYC/16-Prophet-Part-3.png \"Screenshot of Prophet predictor configuration - third section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Alternative, programmatic method for creating legacy predictors**\n",
    ">\n",
    "> If you don't see a UI option to create a legacy predictor (as shown above), you can create one programmatically as follows:\n",
    ">\n",
    "> 1. From the [Dataset groups list](https://console.aws.amazon.com/forecast/home?#datasetGroups), find **your dataset group's ARN** and copy/paste it over the placeholder below\n",
    "> 2. Run the code cell below to create the predictor: After which you should see it appear in your DSG's \"predictors\" list alongside the AutoPredictor we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU CREATED YOUR LEGACY PREDICTOR THROUGH THE AWS CONSOLE, YOU DO NOT NEED TO RUN THIS CELL\n",
    "# (See details above)\n",
    "\n",
    "dataset_group_arn = \"arn:aws:forecast:???:???:dataset-group/???\"  # TODO: Replace with your DSG ARN\n",
    "\n",
    "prophet_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=\"nyctaxi_manual_prophet\",\n",
    "    AlgorithmArn=\"arn:aws:forecast:::algorithm/Prophet\",\n",
    "    ForecastHorizon=168,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters={\n",
    "        \"NumberOfBacktestWindows\": 3,\n",
    "        \"BackTestWindowOffset\": 168,\n",
    "    },\n",
    "    InputDataConfig={\n",
    "        \"DatasetGroupArn\": dataset_group_arn,\n",
    "        \"SupplementaryFeatures\": [\n",
    "            { \"Name\": \"holiday\", \"Value\": \"US\" },\n",
    "            { \"Name\": \"weather\", \"Value\": \"true\" },\n",
    "        ],\n",
    "    },\n",
    "    FeaturizationConfig={\n",
    "        \"ForecastFrequency\": \"H\",\n",
    "        \"ForecastDimensions\": [\"geolocation\"],\n",
    "        \"Featurizations\": [\n",
    "            {\n",
    "                \"AttributeName\": \"target_value\",\n",
    "                \"FeaturizationPipeline\": [\n",
    "                    {\n",
    "                        \"FeaturizationMethodName\": \"filling\",\n",
    "                        \"FeaturizationMethodParameters\": {\n",
    "                            \"frontfill\": \"none\",\n",
    "                            \"middlefill\": \"zero\",\n",
    "                            \"backfill\": \"zero\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Train other predictors\n",
    "\n",
    "You could also explore training Legacy Predictors for other algorithms, such as the neural algorithms DeepAR+ and CNN-QR.\n",
    "\n",
    "However, note that if the AWS Solution Demo is deployed successfully in your account, you'll probably *already have* a trained DeepAR+ predictor in the **pre-created `nyctaxi_weather_auto` dataset group**:\n",
    "\n",
    "![](static/imgs/NYC/17-Pre-Created-DeepAR.png \"Screenshot of automatically created DSG with DeepAR+ predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating predictor accuracy with backtests\n",
    "\n",
    "You might consider holding out the final section of your historical data during preparation, and reconciling produced forecasts to this to calculate model accuracy: A process called [backtesting](https://en.wikipedia.org/wiki/Backtesting).\n",
    "\n",
    "However, Amazon Forecast already performs backtesting internally (to produce [predictor metrics](https://docs.aws.amazon.com/forecast/latest/dg/metrics.html) and compare algorithms) - so you can instead **export the backtest data** to avoid duplicating this effort.\n",
    "\n",
    "Your automatically-created predictor (as shown above) should **already have backtest results exported** - so let's explore those to start.\n",
    "\n",
    "▶️ **Click** On the `nyctaxi_weather_auto` predictor's name hyperlink as shown above, to show the predictor detail page\n",
    "\n",
    "▶️ **Scroll down** to the \"Predictor backtest exports\" section, where you should find an `Active` (completed) job.\n",
    "\n",
    "▶️ **Replace** the dummy S3 uri below with the \"Location\" of this export, and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_s3uri = \"s3://data-bucket-???/exports/???\"  # TODO: Replace with your completed export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](static/imgs/NYC/18-Export-Location.png \"Screenshot showing backtest export location on Predictor details page\")\n",
    "\n",
    "Backtest exports include both detailed accuracy metric breakdowns, and individual predictions versus actual values. Both are partitioned across multiple CSV files in Amazon S3.\n",
    "\n",
    "For example, to explore metric details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_s3_prefix(prefix_s3uri) -> pd.DataFrame:\n",
    "    \"\"\"Function to read all .csv files under an S3 URI prefix to one Pandas DataFrame\n",
    "    \"\"\"\n",
    "    bucket_name, _, prefix = prefix_s3uri[len(\"s3://\"):].partition(\"/\")\n",
    "    prefix_objs = s3.Bucket(bucket_name).objects.filter(Prefix=prefix)\n",
    "    prefix_df = []\n",
    "    for obj in prefix_objs:\n",
    "        key = obj.key\n",
    "        if not key.lower().endswith(\".csv\"):\n",
    "            print(f\"Skipping file {key}\")\n",
    "            continue\n",
    "        body = obj.get()['Body'].read()\n",
    "        df = pd.read_csv(BytesIO(body), encoding='utf8')\n",
    "        prefix_df.append(df)\n",
    "    return pd.concat(prefix_df)\n",
    "\n",
    "\n",
    "backtest_metrics_df = df_from_s3_prefix(f\"{backtest_s3uri}/accuracy-metrics-values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Items by RMSE aggregated across all backtest windows:\\n\")\n",
    "backtest_summaries_by_rmse = backtest_metrics_df[\n",
    "    backtest_metrics_df[\"backtest_window\"] == \"Summary\"\n",
    "].sort_values([\"RMSE\"], ascending=True)\n",
    "\n",
    "print(\"Best:\")\n",
    "display(backtest_summaries_by_rmse.head())\n",
    "\n",
    "print(\"Worst:\")\n",
    "display(backtest_summaries_by_rmse.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or alternatively, to produce plots of predicted values for particular items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_values_df = df_from_s3_prefix(f\"{backtest_s3uri}/forecasted-values\")\n",
    "backtest_values_df[\"timestamp\"] = pd.to_datetime(backtest_values_df[\"timestamp\"])\n",
    "\n",
    "print(\"\\nBacktest window start timestamps:\")\n",
    "backtest_window_starts = backtest_values_df[\"backtestwindow_start_time\"].unique().tolist()\n",
    "print(backtest_window_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_item_id = 79  # Change this to explore different items\n",
    "window_start_time = backtest_window_starts[0]  # Change this to explore different backtests\n",
    "\n",
    "item_backtest_values = backtest_values_df[\n",
    "    (backtest_values_df[\"item_id\"] == target_item_id)\n",
    "    & (backtest_values_df[\"backtestwindow_start_time\"] == window_start_time)\n",
    "].set_index(\"timestamp\")\n",
    "item_backtest_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_forecasts(\n",
    "    item_backtest_values,\n",
    "    actuals=item_backtest_values.rename(columns={ \"target_value\": \"actual\" }),\n",
    "    ylabel=\"Number of Pickups\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own backtest exports\n",
    "\n",
    "Above we explored already-created backtest results from the dataset group that was already set up for you. When your predictors finish training (in `Active` state in the AWS Console), how can you produce such exports yourself?\n",
    "\n",
    "▶️ **Open** the \"Predictors\" list for your dataset group and **Check** that your target predictor is showing in `Active` status (finished training) - and not still `Create in progress` or similar.\n",
    "\n",
    "▶️ **Click** on your predictor's name to open its detail page and then find the **Export backtest results** button a little way down the page:\n",
    "\n",
    "![](static/imgs/NYC/19-Export-Backtest-Button.png \"Screenshot of Prophet Predictor detail page showing 'Export backtest' button\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You'll export your backtests under:\")\n",
    "print(f\"s3://{bucket_name}/exports/[...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Configure** your export as follows and then click \"Start\"\n",
    "\n",
    "- **Name**: Just like dataset import jobs, this needs to be unique. For example, you could use `nyctaxi_manual_prophet_backtest_1`\n",
    "- **IAM Role**: The same IAM Role we created before should be available in the drop-down\n",
    "- **KMS Key**: Can be left blank in this example\n",
    "- **Export Location**: Specify a unique folder under `/exports/` in your data bucket, such as `/exports/nyctaxi_manual_prophet_backtest_1` (No trailing slash)\n",
    "\n",
    "Once your export job shows as \"Active\" in the console (like the pre-created job we saw before), you can re-use the code above to visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Forecasts and forecast exports\n",
    "\n",
    "So far we've evaluated our models based on past data, but what about when you're ready to actually forecast the future?\n",
    "\n",
    "Let's continue with your manually-created Prophet predictor as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Forecast\n",
    "\n",
    "▶️ **Click** \"Create a Forecast\", either by selecting your predictor from the Dataset Group's \"Predictors\" list or from the Dataset Group dashboard.\n",
    "\n",
    "▶️ **Configure** your Forecast as follows and click \"Start\":\n",
    "\n",
    "- **Name**: `nyctaxi_manual_prophet_forecast_1`\n",
    "- **Predictor**: Ensure your `Prophet` predictor is selected\n",
    "- **Quantiles**: Enter `0.1, 0.5, 0.9, mean`\n",
    "\n",
    "![](static/imgs/NYC/20-Create-Prophet-Forecast.png \"Create Forecast configuration for Prophet predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⏰ This operation creates a forward-looking forecast for each item in your dataset and can also take some time. In fact it's actually possible to update DSG datasets and re-forecast *without re-training* a predictor - although generally accuracy will be best if re-training each time.\n",
    "\n",
    "When a Forecast is `Active`, you can query it in real-time using the Amazon Forecast [QueryForecast API](https://docs.aws.amazon.com/forecast/latest/dg/API_forecastquery_QueryForecast.html)\n",
    "\n",
    "(You can use the pre-created forecast in the 'auto' dataset to try out the code below while waiting for your Prophet forecast to complete)\n",
    "\n",
    "> ℹ️ **Note:** The \"Forecast ARN\" required below is listed near the top of the detail page for your particular Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_arn = \"arn:aws:forecast:???:???:forecast/???\"  # TODO: Replace with your Forecast ARN\n",
    "item_id = 79  # Change this to explore different items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcstresult = forecastquery.query_forecast(\n",
    "    ForecastArn=forecast_arn,\n",
    "    Filters={ \"item_id\": str(item_id) },\n",
    ")\n",
    "\n",
    "quantiles = [k for k in fcstresult[\"Forecast\"][\"Predictions\"]]\n",
    "print(f\"Got forecast quantiles {quantiles} for item {item_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_df = pd.DataFrame({\n",
    "    \"timestamp\": [\n",
    "        o[\"Timestamp\"]\n",
    "        for o in fcstresult[\"Forecast\"][\"Predictions\"][quantiles[0]]\n",
    "    ],\n",
    "    \"item_id\": item_id,\n",
    "    **{\n",
    "        k: [o[\"Value\"] for o in fcstresult[\"Forecast\"][\"Predictions\"][k]]\n",
    "        for k in quantiles\n",
    "    },\n",
    "})\n",
    "fcst_df[\"timestamp\"] = pd.to_datetime(fcst_df[\"timestamp\"])\n",
    "fcst_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "display(fcst_df.head())\n",
    "\n",
    "util.plot_forecasts(\n",
    "    fcst_df,\n",
    "    ylabel=\"Number of Pickups\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Forecasts\n",
    "\n",
    "While filtering and querying forecasts in real time through the API may be useful in some cases, other applications may want to export the forecast results in bulk.\n",
    "\n",
    "Once a Forecast is ready, Amazon Forecast supports creating a [Forecast Export Job](https://docs.aws.amazon.com/forecast/latest/dg/API_CreateForecastExportJob.html) to save the results to partitioned CSV files in Amazon S3, much like we saw earlier with backtest exports.\n",
    "\n",
    "> ⚠️ **Note:** Because active forecasts are available through real-time APIs, a [quota limit](https://docs.aws.amazon.com/forecast/latest/dg/limits.html) applies to how many forecasts you can have \"active\" at one time. If you intend to consume your results only via batch processes, you can delete your active Forecasts once their export jobs are complete.\n",
    "\n",
    "When your Forecast is ready, it will show as \"Active\" in the Forecasts list as below:\n",
    "\n",
    "![](static/imgs/NYC/21-Prophet-Forecast-Active.png \"Screenshot of DSG 'Forecasts' list showing active Prophet forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You'll export your forecasts under:\")\n",
    "print(f\"s3://{bucket_name}/exports/[...]\")\n",
    "print(\"(Like we used for backtests earlier)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Select** your forecast from the list and click `Create forecast export` to create an export job.\n",
    "\n",
    "▶️ **Configure** your export job as follows, and click 'Start':\n",
    "\n",
    "- **Name**: `nyctaxi_manual_prophet_forecast_1` - This should be unique, and (since we're using the same S3 folder here) not confusable with backtest results.\n",
    "- **IAM Role**: Same as earlier steps\n",
    "- **Export Location**: Use the job name under the `/exports/` folder as we did earlier: E.g. `/exports/nyctaxi_manual_prophet_forecast_1`.\n",
    "\n",
    "![](static/imgs/NYC/22-Create-Forecast-Export.png \"Screenshot of forecast export job settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the status of Forecast Export jobs can be checked from each Forecast's detail page:\n",
    "\n",
    "![](static/imgs/NYC/23-Forecast-Export-Ongoing.png \"Screenshot of Prophet forecast detail page showing in-progress export job\")\n",
    "\n",
    "When the job shows as \"Active\", the export is complete.\n",
    "\n",
    "As with backtest exports earlier, the result is typically multiple sharded CSV files under your target prefix in S3. While waiting for your export to complete, you could explore the previously-created forecast export in the `nyctaxi_weather_auto` dataset group instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_export_s3uri = \"s3://data-bucket-???/exports/???/\"  # TODO: Replace with your completed export location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_export_df = df_from_s3_prefix(forecast_export_s3uri)\n",
    "forecast_export_df[\"date\"] = pd.to_datetime(forecast_export_df[\"date\"])\n",
    "forecast_export_df.rename(columns={ \"date\": \"timestamp\" }, inplace=True)\n",
    "forecast_export_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 79\n",
    "\n",
    "util.plot_forecasts(\n",
    "    forecast_export_df[forecast_export_df[\"item_id\"] == item_id].set_index(\"timestamp\"),\n",
    "    ylabel=\"Number of Pickups\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up and next steps\n",
    "\n",
    "In this walkthrough we skimmed over actual data preparation and formatting to help you get familiar with actually using the Amazon Forecast service through the AWS Console.\n",
    "\n",
    "As you've probably realised from our work with the auto-created dataset group, it's possible to automate all of these tasks through the [Amazon Forecast APIs](https://docs.aws.amazon.com/forecast/latest/dg/api-reference.html) instead - and the [AWS Solution Implementation 'Improving Forecast Accuracy with Machine Learning'](https://aws.amazon.com/solutions/implementations/improving-forecast-accuracy-with-machine-learning/) provides an example implementation chaining these operations together in a workflow orchestrated by [AWS Step Functions](https://aws.amazon.com/step-functions/). If you open up the [Step Functions Console](https://console.aws.amazon.com/states/home?#/statemachines) you should be able to view the pipeline executed for this automated setup.\n",
    "\n",
    "Now that you have a high-level overview of working with Amazon Forecast, and a reference implementation for automating the workflow, it's likely the most important topic to explore further is how to prepare your data to work correctly with the service and deliver the best model accuracy possible.\n",
    "\n",
    "Good resources for this include:\n",
    "\n",
    "- The [Importing Datasets section of the Amazon Forecast Developer Guide](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-datasets-groups.html)\n",
    "- The [pre-PoC workshop notebooks](https://github.com/aws-samples/amazon-forecast-samples/tree/master/workshops/pre_POC_workshop) on the official [Amazon Forecast Samples GitHub repository](https://github.com/aws-samples/amazon-forecast-samples/tree/master)\n",
    "- The 'Data Diagnostic' notebook in this repository - which can help analyze your TTS for common problems like sparsity of data points."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
