{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Amazon Forecast (Python SDK)\n",
    "\n",
    "> *This notebook should work well in the `Python 3 (Data Science)` kernel in SageMaker Studio, or `conda_python3` in SageMaker Notebook Instances*\n",
    "\n",
    "Now the only mandatory dataset (the TTS) is prepared in compatible CSV format and uploaded to Amazon S3, we're ready to start our experiments with Amazon Forecast.\n",
    "\n",
    "**In this notebook** we'll use the **AWS Python SDK** to:\n",
    "\n",
    "- Create the \"Dataset Group\" wrapper in Forecast to store our project\n",
    "- Define our TTS dataset schema and import the prepared data\n",
    "- Create some **predictors** - training models on our data\n",
    "- Evaluate metrics on how good our predictors seem from the training process\n",
    "- Create and export some **forecasts**\n",
    "\n",
    "Here we use Python commands to automate the various steps. Check out [Notebook 2a](2a.%20Getting%20Started%20with%20Forecast%20(Console).ipynb) for an alternative guide through the same steps using the [Amazon Forecast Console](https://console.aws.amazon.com/forecast/home) instead!\n",
    "\n",
    "Before starting we'll load the required libraries, restore our saved variables from previous notebooks, and establish a connection to the Forecast service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint as prettyprint\n",
    "import secrets\n",
    "import string\n",
    "import time\n",
    "from types import SimpleNamespace  # (because Python dict [\"key\"] notation can get boring)\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region)\n",
    "\n",
    "forecast = session.client(\"forecast\")\n",
    "forecast_query = session.client(\"forecastquery\")\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "export_bucket = s3.Bucket(export_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Dataset Group\n",
    "\n",
    "A **Dataset Group** is the highest level of abstraction in Amazon Forecast, and contains all the source datasets for a particular collection of Forecasts. A Dataset Group contains **up to one of each type** of dataset (Target Time-Series, Related Time-Series, and Item Metadata) and no information is shared between Dataset Groups - so if you'd like to try out various alternatives to the schemas we create below, you could create a new Dataset Group and make your changes inside its corresponding Datasets.\n",
    "\n",
    "To create our Dataset Group, all we need is:\n",
    "\n",
    "- **A name** - We'll create a semi-random name so you can re-run this notebook to create new trials.\n",
    "- **Our chosen [domain](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-domains-ds-types.html)** - here using `CUSTOM` because the traffic forecasting use case doesn't have clear mapping to other predefined domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"forecast_poc_{}\".format(\n",
    "    # Add a random suffix for uniqueness:\n",
    "    \"\".join(secrets.choice(string.ascii_lowercase + string.digits) for i in range(4))\n",
    ")\n",
    "%store project\n",
    "print(f\"Project name '{project}'\")\n",
    "\n",
    "dataset_group_name = project + \"_dsg\"\n",
    "%store dataset_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DatasetGroup\n",
    "create_dataset_group_response = forecast.create_dataset_group(\n",
    "    DatasetGroupName=dataset_group_name,\n",
    "    Domain=\"CUSTOM\",\n",
    ")\n",
    "dataset_group_arn = create_dataset_group_response[\"DatasetGroupArn\"]\n",
    "%store dataset_group_arn\n",
    "print(f\"Created Dataset Group {dataset_group_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the TTS Dataset\n",
    "\n",
    "Next we'll define the **schema** of our TTS dataset, including both the **mandatory** fields for our chosen [domain](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-domains-ds-types.html) and any **optional** fields we've chosen to add.\n",
    "\n",
    "> ⚠️ The schema must **match the prepared data exactly**, *including the order of columns*, because Forecast will validate the data against it when importing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"Attributes\": [\n",
    "        {\n",
    "            \"AttributeName\": \"timestamp\",\n",
    "            \"AttributeType\": \"timestamp\",\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"target_value\",\n",
    "            \"AttributeType\": \"float\",\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"item_id\",\n",
    "            \"AttributeType\": \"string\",\n",
    "        },\n",
    "        # A TTS schema may include extra columns to split the forecast out by other dimensions e.g. location, beyond item_id.\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with this schema, a few other core attributes define our dataset:\n",
    "\n",
    "- The **domain**, which must match our target Dataset Group\n",
    "- The [**frequency**](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-datasets-groups.html#howitworks-data-alignment) (e.g. hourly, daily, etc), which will determine what frequencies of forecasts we can build from our data (e.g. can't build hourly forecasts from weekly source data!)\n",
    "- A **name**, which we'll set in line with our overall project\n",
    "\n",
    "As discussed in the ['Resolving Conflicts in Data Collection Frequency' doc](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-datasets-groups.html#howitworks-data-alignment), the raw CSV data will be automatically mapped and aggregated to the chosen frequency time-steps if it doesn't match already... Just check the right aggregation is being applied in later steps, in case you have any mismatches!\n",
    "\n",
    "![Illustration of Time Step Binning](https://docs.aws.amazon.com/forecast/latest/dg/images/data-alignment.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_dataset(\n",
    "    Domain=\"CUSTOM\",\n",
    "    DatasetType=\"TARGET_TIME_SERIES\",\n",
    "    DatasetName=project + \"_tts\",\n",
    "    DataFrequency=\"H\",  # Hourly data in our case \n",
    "    Schema=schema,\n",
    ")\n",
    "\n",
    "tts_arn = response[\"DatasetArn\"]\n",
    "%store tts_arn\n",
    "print(f\"Created dataset {tts_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset(DatasetArn=tts_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we must **attach** our new dataset to the dataset group, to associate it and have it show in the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the Dataset to the Dataset Group:\n",
    "forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=[tts_arn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the TTS Data\n",
    "\n",
    "We're now ready to populate our Amazon Forecast dataset with the contents of our CSV file from S3.\n",
    "\n",
    "Since this requires the Amazon Forecast service to access the Amazon S3 bucket, this is where we need the *service role* created in Notebook 0: Which has access to the target bucket and trusts the Forecast service. If you don't have such a role set up in your account yet, refer to notebook 0 for details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r forecast_role_arn\n",
    "assert isinstance(forecast_role_arn, str), \"`forecast_role_arn` must be an IAM role ARN (string)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we trigger a **dataset import job**, which is a batch **overwriting** process that clears out any pre-existing data in the dataset: *not* appending data to existing records.\n",
    "\n",
    "Triggering the import requires:\n",
    "\n",
    "- **Naming** the import job, which will be trackable as an entity in the console\n",
    "- Identifying the **target dataset** by its Amazon Resource Name (ARN)\n",
    "- Configuring the **data source**, including the S3 location and also the IAM role used to grant access\n",
    "- Specifying the **timestamp format**, since some variations are permitted according to the [dataset guidelines](https://docs.aws.amazon.com/forecast/latest/dg/dataset-import-guidelines-troubleshooting.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_import_job_response = forecast.create_dataset_import_job(\n",
    "    # You might append a timestamp to the import name in practice, to keep it unique... But here we choose a\n",
    "    # *static* value deliberately, to avoid accidentally & unnecessarily re-importing the PoC data!\n",
    "    DatasetImportJobName=\"poc_import_tts\",\n",
    "    DatasetArn=tts_arn,\n",
    "    DataSource={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": target_s3uri,\n",
    "            \"RoleArn\": forecast_role_arn,\n",
    "        },\n",
    "    },\n",
    "    # (e.g. daily data might omit the hh:mm:ss component)\n",
    "    TimestampFormat=\"yyyy-MM-dd hh:mm:ss\",\n",
    ")\n",
    "\n",
    "ds_import_job_arn = ds_import_job_response[\"DatasetImportJobArn\"]\n",
    "print(ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⏰ The import process can **take a little time** (on the order of ~10-15 minutes for our sample dataset) because of validation, filling & aggregation, and the overhead of spinning up infrastructure to execute the import\n",
    "\n",
    "On small datasets like this, overheads can dominate the run-time and you should expect much better-than-linear scaling as dataset size is increased from this level.\n",
    "\n",
    "Below we'll set up a poll every 30 seconds to wait for the import to complete, after which we'll be ready to use the dataset to train predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_import_status_finished(desc):\n",
    "    status = desc[\"Status\"]\n",
    "    if status == \"ACTIVE\":\n",
    "        return True\n",
    "    elif \"FAILED\" in status:\n",
    "        raise ValueError(f\"Data import failed!\\n{desc}\")\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn),\n",
    "    fn_is_finished=is_import_status_finished,\n",
    "    fn_stringify_result=lambda d: d[\"Status\"],\n",
    "    poll_secs=30,\n",
    "    timeout_secs=60*60,  # Max 1 hour\n",
    ")\n",
    "print(\"Data imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the import is complete, we can also query the API or console UI for some metrics to verify that the data was processed as we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training Predictors\n",
    "\n",
    "As soon as the only mandatory dataset (TTS) is set up and populated with data, we're able to start training forecast models or \"Predictors\".\n",
    "\n",
    "We can create multiple predictors within our Dataset Group, and that's exactly what we'll do in this notebook to compare the results of a few different algorithms offered by the service.\n",
    "\n",
    "Amazon Forecast offers 6 (at the time of writing) algorithms as described in more detail on the [\"Choosing an Algorithm\" doc page](https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-choosing-recipes.html) - which we've grouped here into 3 rough categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline/statistical methods:\n",
    "arima_algorithm_arn = \"arn:aws:forecast:::algorithm/ARIMA\"\n",
    "ets_algorithm_arn = \"arn:aws:forecast:::algorithm/ETS\"\n",
    "\n",
    "# Probabilistic methods:\n",
    "npts_algorithm_arn = \"arn:aws:forecast:::algorithm/NPTS\"\n",
    "prophet_algorithm_arn = \"arn:aws:forecast:::algorithm/Prophet\"\n",
    "\n",
    "# Deep learning methods:\n",
    "deeparp_algorithm_arn = \"arn:aws:forecast:::algorithm/Deep_AR_Plus\"\n",
    "cnnqr_algorithm_arn = \"arn:aws:forecast:::algorithm/CNN-QR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although an **AutoML** option is available which will automatically try each algorithm, list the metrics of each, and keep the best model; we'll create a set of models manually in this example so that we're able to generate a forecast output for each, to plot and compare.\n",
    "\n",
    "As well as the algorithm (by ARN), we'll need to specify when creating our predictors:\n",
    "\n",
    "- A **Name**, which we'll use to identify which algorithm is used in each experiment\n",
    "- The **Forecast Frequency**, which must not be less than the dataset (e.g. hourly, daily, etc.)\n",
    "- The **Forecast Horizon** in terms of this frequency, which must not be more than 500 or 1/3 of the dataset length per the [quotas page](https://docs.aws.amazon.com/forecast/latest/dg/limits.html)\n",
    "- Whether to use the **Built-In Holiday Dataset** for any country, to augment the data\n",
    "- How to **Backtest** the model for accuracy metrics from the training data, as described in the [\"Evaluating Predictor Accuracy\" doc page](https://docs.aws.amazon.com/forecast/latest/dg/metrics.html)\n",
    "- The **Featurization** configuration for how to [handle missing values](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-missing-values.html) in each field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_frequency = \"H\"\n",
    "forecast_horizon = 240\n",
    "\n",
    "evaluation_parameters = {\n",
    "    \"NumberOfBacktestWindows\": 1,\n",
    "    \"BackTestWindowOffset\": 240,\n",
    "}\n",
    "\n",
    "input_data_config = {\n",
    "    \"DatasetGroupArn\": dataset_group_arn,\n",
    "    \"SupplementaryFeatures\": [\n",
    "        { \"Name\": \"holiday\", \"Value\": \"US\" },\n",
    "    ],\n",
    "}\n",
    "\n",
    "featurization_config = {\n",
    "    \"ForecastFrequency\": forecast_frequency,\n",
    "    \"Featurizations\": [\n",
    "        {\n",
    "            \"AttributeName\": \"target_value\",\n",
    "            \"FeaturizationPipeline\": [\n",
    "                {\n",
    "                    \"FeaturizationMethodName\": \"filling\",\n",
    "                    \"FeaturizationMethodParameters\": {\n",
    "                        \"frontfill\": \"none\",\n",
    "                        \"middlefill\": \"zero\",\n",
    "                        \"backfill\": \"zero\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we'll set up a top-level dictionary to store the results from our experiments and then kick off each Predictor's training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=f\"{project}_arima_algo_1\",\n",
    "    AlgorithmArn=arima_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters=evaluation_parameters,\n",
    "    InputDataConfig=input_data_config,\n",
    "    FeaturizationConfig=featurization_config,\n",
    ")\n",
    "results[\"ARIMA\"] = SimpleNamespace(predictor_arn=arima_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=f\"{project}_prophet_algo_1\",\n",
    "    AlgorithmArn=prophet_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters=evaluation_parameters,\n",
    "    InputDataConfig=input_data_config,\n",
    "    FeaturizationConfig=featurization_config,\n",
    ")\n",
    "results[\"Prophet\"] = SimpleNamespace(predictor_arn=prophet_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeparp_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=f\"{project}_deeparp_algo_1\",\n",
    "    AlgorithmArn=deeparp_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters=evaluation_parameters,\n",
    "    InputDataConfig=input_data_config,\n",
    "    FeaturizationConfig=featurization_config,\n",
    ")\n",
    "results[\"DeepAR+\"] = SimpleNamespace(predictor_arn=deeparp_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR\n",
    "\n",
    "We've commented out CNN-QR sections because in our tests the algorithm takes longer to train on the small sample dataset than DeepAR+ - with comparable accuracy. On many larger \"real\" datasets CNN-QR can be much faster, so we'd recommend experimenting with it on your own data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnnqr_create_predictor_response = forecast.create_predictor(\n",
    "#     PredictorName=f\"{project}_cnnqr_algo_1\",\n",
    "#     AlgorithmArn=cnnqr_algorithm_arn,\n",
    "#     ForecastHorizon=forecast_horizon,\n",
    "#     PerformAutoML=False,\n",
    "#     PerformHPO=False,\n",
    "#     EvaluationParameters=evaluation_parameters,\n",
    "#     InputDataConfig=input_data_config,\n",
    "#     FeaturizationConfig=featurization_config,\n",
    "# )\n",
    "# results[\"CNN-QR\"] = SimpleNamespace(predictor_arn=cnnqr_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally in our notebooks we would have a while loop that polls for each of these to determine the status of the models in training. The cell below will poll for the ARNs of each and return when they are all available so you can move on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_predictors = [results[r].predictor_arn for r in results]\n",
    "failed_predictors = []\n",
    "\n",
    "def check_status():\n",
    "    \"\"\"Check and update in_progress_predictors\"\"\"\n",
    "    just_stopped = []  # Can't edit the in_progress list directly the loop!\n",
    "    for arn in in_progress_predictors:\n",
    "        predictor_desc = forecast.describe_predictor(PredictorArn=arn)\n",
    "        status = predictor_desc[\"Status\"]\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f\"\\nBuild succeeded for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "        elif \"FAILED\" in status:\n",
    "            print(f\"\\nBuild failed for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "            failed_predictors.append(arn)\n",
    "    for arn in just_stopped:\n",
    "        in_progress_predictors.remove(arn)\n",
    "    return in_progress_predictors\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=check_status,\n",
    "    fn_is_finished=lambda l: len(l) == 0,\n",
    "    fn_stringify_result=lambda l: f\"{len(l)} predictor builds in progress\",\n",
    "    poll_secs=60,  # Poll every minute\n",
    "    timeout_secs=3*60*60,  # Max 3 hours\n",
    ")\n",
    "\n",
    "if len(failed_predictors):\n",
    "    raise RuntimeError(f\"The following predictors failed to train:\\n{failed_predictors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Predictors\n",
    "\n",
    "Once each of the Predictors is in an `Active` state you can get metrics about it to better understand its accuracy and behavior. These are computed based on the hold out periods we defined when building the Predictor. The metrics are meant to guide our decisions when we use a particular Predictor to generate a forecast.\n",
    "\n",
    "Below we'll define a utility function below which retrieves (and prints) the raw accuracy metrics response, and also builds up a leaderboard. In the following cells, we'll run the function against each trained predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trial_metrics(trial_name=None) -> pd.DataFrame:\n",
    "    \"\"\"Utility to fetch the accuracy metrics for a predictor and output the leaderboard so far\"\"\"\n",
    "    if (trial_name):\n",
    "        # Print the raw API response:\n",
    "        metrics_response = forecast.get_accuracy_metrics(PredictorArn=results[trial_name].predictor_arn)\n",
    "        print(f\"Raw metrics for {trial_name}:\")\n",
    "        prettyprint(metrics_response)\n",
    "\n",
    "        # Save the payload section to results:\n",
    "        evaluation_results = metrics_response[\"PredictorEvaluationResults\"]\n",
    "        results[trial_name].evaluation_results = evaluation_results\n",
    "\n",
    "        # Construct simplified version for our comparison:\n",
    "        try:\n",
    "            summary_metrics = next(\n",
    "                w for w in evaluation_results[0][\"TestWindows\"] if w[\"EvaluationType\"] == \"SUMMARY\"\n",
    "            )[\"Metrics\"]\n",
    "        except StopIteration:\n",
    "            raise ValueError(\"Couldn't find SUMMARY metrics in Forecast API response\")\n",
    "        results[trial_name].summary_metrics = {\n",
    "            \"RMSE\": summary_metrics[\"RMSE\"],\n",
    "            \"10% wQL\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.1\n",
    "            ),\n",
    "            \"50% wQL (MAPE)\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.5\n",
    "            ),\n",
    "            \"90% wQL\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.9\n",
    "            ),\n",
    "        }\n",
    "    # Render the leaderboard:\n",
    "    return pd.DataFrame([\n",
    "        { \"Predictor\": name, **results[name].summary_metrics } for name in results\n",
    "        if \"summary_metrics\" in results[name].__dict__\n",
    "    ]).set_index(\"Predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA\n",
    "\n",
    "ARIMA is one of the gold standards for time series forecasting. This algorithm is not particularly sophisticated but it is reliable and can help us understand a baseline of performance. To note it does not really understand seasonality very well and it does not support any item metadata or related time series information. Due to that we will explore it here but not after adding other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trial_metrics(\"ARIMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our test, ARIMA scored a RMSE of ~1950 and 50% weighted quantile loss (=MAPE) of ~0.4715. ARIMA results will help us benchmark other predictors, looking for a reduction versus these baseline loss figures. Your results may vary a little across the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet\n",
    "\n",
    "As with ARIMA, let's explore the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trial_metrics(\"Prophet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our test, Prophet achieved a slightly lower RMSE than ARIMA at around 1910 - but the weighted quantile losses were pretty similar and worse on some quantiles.\n",
    "\n",
    "At this point, Prophet has not had a chance to use any of its abilities to integrate related time-series information since only the target time-series has been uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+\n",
    "\n",
    "As with Prophet and ARIMA, let's explore the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trial_metrics(\"DeepAR+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our test, DeepAR+ achieved a significant improvement in accuracy as measured both by RMSE (~1570) and at the 50% and 90% quantiles. The 10% quantile showed somewhat poorer performance, but overall accuracy was still good.\n",
    "\n",
    "To see what all this looks like in a visual format, we'll now create a Forecast with each Predictor and then export them to S3 to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_trial_metrics(\"CNN-QR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Forecasts\n",
    "\n",
    "Inside Amazon Forecast a Forecast is a rendered collection of all of your items, at every time interval, for all selected quantiles, for your given forecast horizon. This process takes the Predictor you just created and uses it to generate these inferences and to store them in a useful state. Once a Forecast exists within the service you can query it and obtain a JSON response or use another API call to export it to a CSV that is stored in S3.\n",
    "\n",
    "First, we'll *create* forecasts from our models using the commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response = forecast.create_forecast(\n",
    "    ForecastName=f\"{project}_arima_algo_forecast\",\n",
    "    PredictorArn=results[\"ARIMA\"].predictor_arn,\n",
    ")\n",
    "results[\"ARIMA\"].forecast_arn = create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response = forecast.create_forecast(\n",
    "    ForecastName=f\"{project}_prophet_algo_forecast\",\n",
    "    PredictorArn=results[\"Prophet\"].predictor_arn,\n",
    ")\n",
    "results[\"Prophet\"].forecast_arn = create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response = forecast.create_forecast(\n",
    "    ForecastName=f\"{project}_deeparp_algo_forecast\",\n",
    "    PredictorArn=results[\"DeepAR+\"].predictor_arn,\n",
    ")\n",
    "results[\"DeepAR+\"].forecast_arn = create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_forecast_response = forecast.create_forecast(\n",
    "#     ForecastName=f\"{project}_cnnqr_algo_forecast\",\n",
    "#     PredictorArn=results[\"CNN-QR\"].predictor_arn,\n",
    "# )\n",
    "# results[\"CNN-QR\"].forecast_arn = create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again as you saw in the training step, you should poll until completion to know that you are ready to proceed to the next step. The cell below will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_forecasts = [results[r].forecast_arn for r in results]\n",
    "failed_forecasts = []\n",
    "\n",
    "def check_status():\n",
    "    \"\"\"Check and update in_progress_forecasts\"\"\"\n",
    "    just_stopped = []  # Can't edit the in_progress list directly the loop!\n",
    "    for arn in in_progress_forecasts:\n",
    "        desc_response = forecast.describe_forecast(ForecastArn=arn)\n",
    "        status = desc_response[\"Status\"]\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f\"\\nBuild succeeded for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "        elif \"FAILED\" in status:\n",
    "            print(f\"\\nBuild failed for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "            failed_forecasts.append(arn)\n",
    "    for arn in just_stopped:\n",
    "        in_progress_forecasts.remove(arn)\n",
    "    return in_progress_forecasts\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=check_status,\n",
    "    fn_is_finished=lambda l: len(l) == 0,\n",
    "    fn_stringify_result=lambda l: f\"{len(l)} forecast builds in progress\",\n",
    "    poll_secs=60,  # Poll every 60s\n",
    "    timeout_secs=2*60*60,  # Max 2 hours\n",
    ")\n",
    "\n",
    "if len(failed_forecasts):\n",
    "    raise RuntimeError(f\"The following forecasts failed:\\n{failed_forecasts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Forecasts\n",
    "\n",
    "Although forecasts may be either queried directly through the API (or the *Forecast lookup* tab of the [Amazon Forecast Console](https://console.aws.amazon.com/forecast/home)), **exporting** the forecast to S3 bucket is also possible and we'll use this method to visualize our results in this notebook.\n",
    "\n",
    "Once the forecasts have entered `Active` status, they are ready to be exported. Below, we create **export jobs** for each forecast in our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_locations = {\n",
    "    \"ARIMA\": f\"s3://{export_bucket_name}/exports/arima/tts-only\",\n",
    "    \"Prophet\": f\"s3://{export_bucket_name}/exports/prophet/tts-only\",\n",
    "    \"DeepAR+\": f\"s3://{export_bucket_name}/exports/deeparp/tts-only\",\n",
    "    #\"CNN-QR\": f\"s3://{export_bucket_name}/exports/cnnqr/tts-only\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=\"arima_export\",\n",
    "    ForecastArn=results[\"ARIMA\"].forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": export_locations[\"ARIMA\"],\n",
    "            \"RoleArn\": forecast_role_arn,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "results[\"ARIMA\"].export_arn = export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=\"prophet_export\",\n",
    "    ForecastArn=results[\"Prophet\"].forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": export_locations[\"Prophet\"],\n",
    "            \"RoleArn\": forecast_role_arn,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "results[\"Prophet\"].export_arn = export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=\"deeparp_export\",\n",
    "    ForecastArn=results[\"DeepAR+\"].forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": export_locations[\"DeepAR+\"],\n",
    "            \"RoleArn\": forecast_role_arn,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "results[\"DeepAR+\"].export_arn = export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_response = forecast.create_forecast_export_job(\n",
    "#     ForecastExportJobName=\"cnnqr_export\",\n",
    "#     ForecastArn=results[\"CNN-QR\"].forecast_arn,\n",
    "#     Destination={\n",
    "#         \"S3Config\": {\n",
    "#             \"Path\": export_locations[\"CNN-QR\"],\n",
    "#             \"RoleArn\": forecast_role_arn,\n",
    "#         },\n",
    "#     },\n",
    "# )\n",
    "# results[\"CNN-QR\"].export_arn = export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exporting process is another one of those items that will take several minutes to complete. Once again, poll with the cell below then move on to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_exports = [results[r].export_arn for r in results]\n",
    "failed_exports = []\n",
    "\n",
    "def check_status():\n",
    "    \"\"\"Check and update in_progress_exports\"\"\"\n",
    "    just_stopped = []  # Can't edit the in_progress list directly the loop!\n",
    "    for arn in in_progress_exports:\n",
    "        desc_response = forecast.describe_forecast_export_job(ForecastExportJobArn=arn)\n",
    "        status = desc_response[\"Status\"]\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f\"\\nExport succeeded for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "        elif \"FAILED\" in status:\n",
    "            print(f\"\\nExport failed for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "            failed_exports.append(arn)\n",
    "    for arn in just_stopped:\n",
    "        in_progress_exports.remove(arn)\n",
    "    return in_progress_exports\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=check_status,\n",
    "    fn_is_finished=lambda l: len(l) == 0,\n",
    "    fn_stringify_result=lambda l: f\"{len(l)} forecast exports in progress\",\n",
    "    poll_secs=20,  # Poll every 20s\n",
    "    timeout_secs=60*60,  # Max 1 hour\n",
    ")\n",
    "\n",
    "if len(failed_exports):\n",
    "    raise RuntimeError(f\"The following exports failed:\\n{failed_exports}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that it's also possible to retrieve the output location for a completed export by ARN:\n",
    "job_desc = forecast.describe_forecast_export_job(ForecastExportJobArn=results[\"Prophet\"].export_arn)\n",
    "s3uri = job_desc[\"Destination\"][\"S3Config\"][\"Path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Now it's time to explore the exported results - downloading the CSV files and plotting the forecasts against our held-out validation data to check the format and forecasts are as expected.\n",
    "\n",
    "> ⚠️ **Note:** This first cell is provided for those who have followed the AWS Console notebook 2a and are picking up this notebook here - you don't need to run it if you've already run the earlier sections of this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catch-up cell for console tutorial users joining here for visualization:\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint as prettyprint\n",
    "import secrets\n",
    "import string\n",
    "import time\n",
    "from types import SimpleNamespace  # (because Python dict [\"key\"] notation can get boring)\n",
    "\n",
    "import boto3\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "\n",
    "import util\n",
    "\n",
    "%store -r\n",
    "\n",
    "\n",
    "# CHECK that these locations match yours!\n",
    "export_locations = {\n",
    "    \"ARIMA\": f\"s3://{export_bucket_name}/exports/arima/tts-only\",\n",
    "    \"Prophet\": f\"s3://{export_bucket_name}/exports/prophet/tts-only\",\n",
    "    \"DeepAR+\": f\"s3://{export_bucket_name}/exports/deeparp/tts-only\",\n",
    "    #\"CNN-QR\": f\"s3://{export_bucket_name}/exports/cnnqr/tts-only\",\n",
    "}\n",
    "print(\"Forecast exports at:\\n\" + json.dumps(export_locations, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Forecasts\n",
    "\n",
    "To load and visualize the forecasts in our notebook, we first need to download the files from S3.\n",
    "\n",
    "Although [Boto3 S3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html) provides functions for listing and fetching files, we'll simplify the process a bit by calling the `aws s3 sync` command from the [AWS CLI](https://aws.amazon.com/cli/).\n",
    "\n",
    "> ⚠️ **Note:** Large exports may split output into multiple files, so here we store *lists* of filenames for each export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_filenames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3uri = export_locations[\"ARIMA\"]\n",
    "local_folder = f\"data/exports/arima/tts-only\"\n",
    "\n",
    "!aws s3 sync $s3uri $local_folder\n",
    "\n",
    "forecast_filenames[\"ARIMA\"] = util.list_files_with_extension(\n",
    "    local_folder,\n",
    "    ext=\"csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3uri = export_locations[\"Prophet\"]\n",
    "local_folder = f\"data/exports/prophet/tts-only\"\n",
    "\n",
    "!aws s3 sync $s3uri $local_folder\n",
    "\n",
    "forecast_filenames[\"Prophet\"] = util.list_files_with_extension(\n",
    "    local_folder,\n",
    "    ext=\"csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3uri = export_locations[\"DeepAR+\"]\n",
    "local_folder = f\"data/exports/deeparp/tts-only\"\n",
    "\n",
    "!aws s3 sync $s3uri $local_folder\n",
    "\n",
    "forecast_filenames[\"DeepAR+\"] = util.list_files_with_extension(\n",
    "    local_folder,\n",
    "    ext=\"csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3uri = export_locations[\"CNN-QR\"]\n",
    "# local_folder = f\"data/exports/cnn-qr/tts-only\"\n",
    "\n",
    "# !aws s3 sync $s3uri $local_folder\n",
    "\n",
    "# forecast_filenames[\"CNN-QR\"] = util.list_files_with_extension(\n",
    "#     local_folder,\n",
    "#     ext=\"csv\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the forecast CSVs are downloaded, we're ready to plot them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA\n",
    "\n",
    "We'll take things slowly with our first (ARIMA) forecast to demonstrate the details, and then speed things up for the others.\n",
    "\n",
    "First, let's take a look at the raw DataFrame as loaded by Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts = util.read_multipart_csv(forecast_filenames[\"ARIMA\"])\n",
    "arima_predicts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tidy things up a bit by parsing the timestamps and indexing the dataframe by them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to datetime\n",
    "arima_predicts[\"date\"] = pd.to_datetime(arima_predicts[\"date\"])\n",
    "arima_predicts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the timezone and make date the index\n",
    "arima_predicts['date'] = arima_predicts['date'].dt.tz_convert(None)\n",
    "arima_predicts.set_index('date', inplace=True)\n",
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arima_predicts.index.min())\n",
    "print(arima_predicts.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see our prediction goes from Jan 01 to Jan 10 as expected given our 240 interval forecast horizon. Also we can see the cyclical nature of the predictions over the entire timeframe.\n",
    "\n",
    "To visualize our forecast against the validation data, it would be helpful to:\n",
    "\n",
    "- Remove/deal with the `item_id` column (which is always constant for our our single-item sample data)\n",
    "- Overlay the actual value from the validation data\n",
    "\n",
    "Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut out item_id:\n",
    "arima_simple = arima_predicts[['p10', 'p50', 'p90']]\n",
    "arima_simple.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the time window of validation data we'd like to overlay:\n",
    "validation_df = validation_time_series_df.rename(columns={\"traffic_volume\": \"actual\"}).loc[\"2018-01-01\":\"2018-01-10\"]\n",
    "print(validation_df.index.min())\n",
    "print(validation_df.index.max())\n",
    "validation_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the dataframes together:\n",
    "arima_val_df = arima_simple.join(validation_df, how=\"outer\")\n",
    "arima_val_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this particular plot is hard to see, let us pick a random day January 5th to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_val_df.loc[\"2018-01-05\":\"2018-01-06\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the actual traffic tracks quite closely to the p50 median prediction, and should stay within the p10-p90 band for most or all of the forecast horizon.\n",
    "\n",
    "As a final step, we've implemented a utility function to improve the clarity a little further by expanding the plot area and plotting the p10/p90 interval as a **confidence interval** rather than independent lines.\n",
    "\n",
    "Check you're comfortable with the plot below, and then we'll move on to comparing ARIMA with our other predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_forecasts(arima_predicts, actuals=validation_df, ylabel=\"Traffic Volume\")\n",
    "\n",
    "util.plot_forecasts(\n",
    "    arima_predicts[\"2018-01-05\":\"2018-01-06\"],\n",
    "    actuals=validation_df[\"2018-01-05\":\"2018-01-06\"],\n",
    "    ylabel=\"Traffic Volume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join([\"Loading...\"] + forecast_filenames[\"Prophet\"]))\n",
    "prophet_predicts = util.read_multipart_csv(forecast_filenames[\"Prophet\"])\n",
    "\n",
    "# Set up date index:\n",
    "prophet_predicts[\"date\"] = pd.to_datetime(prophet_predicts[\"date\"]).dt.tz_convert(None)\n",
    "prophet_predicts.set_index(\"date\", inplace=True)\n",
    "\n",
    "util.plot_forecasts(prophet_predicts, actuals=validation_df, ylabel=\"Traffic Volume\")\n",
    "\n",
    "util.plot_forecasts(\n",
    "    prophet_predicts.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "    actuals=validation_df.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "    ylabel=\"Traffic Volume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join([\"Loading...\"] + forecast_filenames[\"DeepAR+\"]))\n",
    "deeparp_predicts = util.read_multipart_csv(forecast_filenames[\"DeepAR+\"])\n",
    "\n",
    "# Set up date index:\n",
    "deeparp_predicts[\"date\"] = pd.to_datetime(deeparp_predicts[\"date\"]).dt.tz_convert(None)\n",
    "deeparp_predicts.set_index(\"date\", inplace=True)\n",
    "\n",
    "util.plot_forecasts(deeparp_predicts, actuals=validation_df, ylabel=\"Traffic Volume\")\n",
    "\n",
    "util.plot_forecasts(\n",
    "    deeparp_predicts.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "    actuals=validation_df.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "    ylabel=\"Traffic Volume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is particularly interesting here is that even the p90 prediction is significantly below the actual numbers for a good portion of some days.\n",
    "\n",
    "Consider how the evaluation metrics of these algorithms relate to the observed performance characteristics in the validation plots. Clearly for probabilistic forecasts, central metrics like RMSE and MAPE/wQL0.5 tell only part of the story of model accuracy - and this is why additional quantile loss metrics are presented.\n",
    "\n",
    "Note that different algorithms generate quantiles by different methods. In particular, CNN-QR does not guarantee the ordering of quantiles (although good-quality fits should converge towards quantiles being ordered) - so there might be brief periods where e.g. the `p10` forecast quantile is higher than `p50`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n\".join([\"Loading...\"] + forecast_filenames[\"CNN-QR\"]))\n",
    "# prophet_predicts = util.read_multipart_csv(forecast_filenames[\"CNN-QR\"])\n",
    "\n",
    "# # Set up date index:\n",
    "# cnnqr_predicts[\"date\"] = pd.to_datetime(cnnqr_predicts[\"date\"]).dt.tz_convert(None)\n",
    "# cnnqr_predicts.set_index(\"date\", inplace=True)\n",
    "\n",
    "# util.plot_forecasts(cnnqr_predicts, actuals=validation_df, ylabel=\"Traffic Volume\")\n",
    "\n",
    "# util.plot_forecasts(\n",
    "#     cnnqr_predicts.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "#     actuals=validation_df.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "#     ylabel=\"Traffic Volume\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap and Next Steps\n",
    "\n",
    "We've now explored some initial models on the target timeseries alone, and can start exploring additional **related data** as a way to improve forecast accuracy. The [next notebook, #3](3.%20Preparing%20Related%20Time-Series%20Data.ipynb) will guide you through the process of preparing a *related time-series* file ready to upload to Amazon Forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
