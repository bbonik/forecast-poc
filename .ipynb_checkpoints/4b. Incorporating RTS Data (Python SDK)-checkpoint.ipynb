{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating RTS Data (Python SDK)\n",
    "\n",
    "> *This notebook should work well in the `Python 3 (Data Science)` kernel in SageMaker Studio, or `conda_python3` in SageMaker Notebook Instances*\n",
    "\n",
    "**In this notebook** we'll use the **AWS Python SDK** to:\n",
    "\n",
    "- Import the prepared *Related Time-Series* data from notebook 3 to our existing Amazon Forecast *Dataset Group*\n",
    "- Train new predictors and generate forecasts using the additional data\n",
    "- Explore how the extra information affects our forecast quality\n",
    "\n",
    "Check out Notebook [4a. Incorporating RTS Data (Console)](4a.%20Incorporating%20RTS%20Data%20(Console).ipynb) for an alternative guide through the same steps using the [Amazon Forecast Console](https://console.aws.amazon.com/forecast/home) instead!\n",
    "\n",
    "Before starting we'll load the required libraries, restore our saved variables from previous notebooks, and establish a connection to the Forecast service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "from pprint import pprint as prettyprint\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region)\n",
    "\n",
    "forecast = session.client(\"forecast\")\n",
    "forecast_query = session.client(\"forecastquery\")\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "export_bucket = s3.Bucket(export_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the RTS Dataset\n",
    "\n",
    "Since our Dataset Group is already created, the first step will be to add Related Time-Series data will be to define the structure of the dataset.\n",
    "\n",
    "We'll define the **data schema** as below (check this matches the data as shown at the end of the RTS preparation notebook!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Make sure the order of columns matches the data files!)\n",
    "related_schema = {\n",
    "    \"Attributes\": [\n",
    "        {\n",
    "            \"AttributeName\": \"timestamp\",\n",
    "            \"AttributeType\": \"timestamp\",\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"temperature\",\n",
    "            \"AttributeType\": \"float\",\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"rain_1h\",\n",
    "            \"AttributeType\": \"float\",\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"snow_1h\",\n",
    "            \"AttributeType\": \"float\",\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"clouds_all\",\n",
    "            \"AttributeType\": \"float\",\n",
    "        },\n",
    "        {\n",
    "            \"AttributeName\": \"item_id\",\n",
    "            \"AttributeType\": \"string\",\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, this schema together with a **name**, **frequency** and **domain** will define the dataset entity in the Forecast service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_dataset(\n",
    "    Domain=\"CUSTOM\",\n",
    "    DatasetType=\"RELATED_TIME_SERIES\",\n",
    "    DatasetName=project + \"_rts\",\n",
    "    DataFrequency=\"H\", \n",
    "    Schema=related_schema,\n",
    ")\n",
    "\n",
    "rts_arn = response[\"DatasetArn\"]\n",
    "%store rts_arn\n",
    "print(f\"Created dataset {rts_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset(DatasetArn=rts_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And we must **attach** our new dataset to the dataset group, to associate it and have it show in the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.update_dataset_group(\n",
    "    # If you don't have your dataset group ARN (perhaps because you created it via the console), you can set it given\n",
    "    # your AWS Account ID, region, and dataset group name - something like this:\n",
    "    # DatasetGroupArn=\"arn:aws:forecast:ap-southeast-1:123456789012:dataset-group/forecast_poc\"\n",
    "    DatasetGroupArn=dataset_group_arn,\n",
    "    # If you'd like to temporarily un-link the RTS from your dataset group and then bring it back again, this command\n",
    "    # can be a useful tool!\n",
    "    DatasetArns=[tts_arn, rts_arn],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the RTS Data\n",
    "\n",
    "We're now ready to populate our Amazon Forecast dataset with the contents of our CSV file from S3.\n",
    "\n",
    "Since this requires the Amazon Forecast service to access the Amazon S3 bucket, this is where we need the service role created in Notebook 0: Which has access to the target bucket and trusts the Forecast service. If you don't have such a role set up in your account yet, refer to notebook 0 for details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r forecast_role_arn\n",
    "assert isinstance(forecast_role_arn, str), \"`forecast_role_arn` must be an IAM role ARN (string)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we trigger a **dataset import job**, which is a batch **overwriting** process that clears out any pre-existing data in the dataset: *not* appending data to existing records.\n",
    "\n",
    "Triggering the import requires:\n",
    "\n",
    "- **Naming** the import job, which will be trackable as an entity in the console\n",
    "- Identifying the **target dataset** by its Amazon Resource Name (ARN)\n",
    "- Configuring the **data source**, including the S3 location and also the IAM role used to grant access\n",
    "- Specifying the **timestamp format**, since some variations are permitted according to the [dataset guidelines](https://docs.aws.amazon.com/forecast/latest/dg/dataset-import-guidelines-troubleshooting.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_import_job_response = forecast.create_dataset_import_job(\n",
    "    # You might append a timestamp to the import name in practice, to keep it unique... But here we choose a\n",
    "    # *static* value deliberately, to avoid accidentally & unnecessarily re-importing the PoC data!\n",
    "    DatasetImportJobName=\"poc_import_rts\",\n",
    "    DatasetArn=rts_arn,\n",
    "    DataSource={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": related_s3uri,\n",
    "            \"RoleArn\": forecast_role_arn,\n",
    "        },\n",
    "    },\n",
    "    # (e.g. daily data might omit the hh:mm:ss component)\n",
    "    TimestampFormat=\"yyyy-MM-dd hh:mm:ss\",\n",
    ")\n",
    "\n",
    "rts_import_job_arn = rts_import_job_response[\"DatasetImportJobArn\"]\n",
    "print(rts_import_job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⏰ The import process can **take a little time** (on the order of ~10-15 minutes for our sample dataset) because of validation, filling & aggregation, and the overhead of spinning up infrastructure to execute the import\n",
    "\n",
    "On small datasets like this, overheads can dominate the run-time and you should expect much better-than-linear scaling as dataset size is increased from this level.\n",
    "\n",
    "As before with TTS, we'll set up a poll to check the status of the import and wait to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_import_status_finished(desc):\n",
    "    status = desc[\"Status\"]\n",
    "    if status == \"ACTIVE\":\n",
    "        return True\n",
    "    elif status == \"CREATE_FAILED\":\n",
    "        raise ValueError(f\"Data import failed!\\n{desc}\")\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=rts_import_job_arn),\n",
    "    fn_is_finished=is_import_status_finished,\n",
    "    fn_stringify_result=lambda d: d[\"Status\"],\n",
    "    poll_secs=30,  # Poll every 30s\n",
    "    timeout_secs=60*60,  # Max 1 hour\n",
    ")\n",
    "print(\"Data imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an added check, we may use the `DescribeDatasetImportJob` API to verify the results of the import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_import_job(DatasetImportJobArn=rts_import_job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training Predictors\n",
    "\n",
    "As before, we'll fix most of the parameters for our forecast and create models to compare the performance of different algorithms: This time just testing Prophet and DeepAR+ (and adding CNN-QR if you like), since ARIMA is not capable of utilizing extra RTS information.\n",
    "\n",
    "We'll first re-define the RTS-aware Algorithm ARNs for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_algorithm_arn = \"arn:aws:forecast:::algorithm/Prophet\"\n",
    "deeparp_algorithm_arn = \"arn:aws:forecast:::algorithm/Deep_AR_Plus\"\n",
    "cnnqr_algorithm_arn = \"arn:aws:forecast:::algorithm/CNN-QR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and declare other static forecast configurations the same as we did for notebook 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_frequency = \"H\"\n",
    "forecast_horizon = 240\n",
    "\n",
    "evaluation_parameters = {\n",
    "    \"NumberOfBacktestWindows\": 1,\n",
    "    \"BackTestWindowOffset\": 240,\n",
    "}\n",
    "\n",
    "input_data_config = {\n",
    "    \"DatasetGroupArn\": dataset_group_arn,\n",
    "    \"SupplementaryFeatures\": [\n",
    "        { \"Name\": \"holiday\", \"Value\": \"US\" },\n",
    "    ],\n",
    "}\n",
    "\n",
    "featurization_config = {\n",
    "    \"ForecastFrequency\": forecast_frequency,\n",
    "    \"Featurizations\": [\n",
    "        {\n",
    "            \"AttributeName\": \"target_value\",\n",
    "            \"FeaturizationPipeline\": [\n",
    "                {\n",
    "                    \"FeaturizationMethodName\": \"filling\",\n",
    "                    \"FeaturizationMethodParameters\": {\n",
    "                        \"frontfill\": \"none\",\n",
    "                        \"middlefill\": \"zero\",\n",
    "                        \"backfill\": \"zero\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `results` dictionary should already have been declared in notebook 2b (but we'll create a new one if not), so we're now ready to create our new RTS-aware predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if results:\n",
    "        print(\"Found existing 'results' dictionary\")\n",
    "except NameError:\n",
    "    results = {}\n",
    "    print(\"No existing results found - created new dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=f\"{project}_prophet_rts\",\n",
    "    AlgorithmArn=prophet_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters=evaluation_parameters,\n",
    "    InputDataConfig=input_data_config,\n",
    "    FeaturizationConfig=featurization_config,\n",
    ")\n",
    "results[\"Prophet with RTS\"] = SimpleNamespace(predictor_arn=prophet_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeparp_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=f\"{project}_deeparp_rts\",\n",
    "    AlgorithmArn=deeparp_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters=evaluation_parameters,\n",
    "    InputDataConfig=input_data_config,\n",
    "    FeaturizationConfig=featurization_config,\n",
    ")\n",
    "results[\"DeepAR+ with RTS\"] = SimpleNamespace(predictor_arn=deeparp_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnnqr_create_predictor_response = forecast.create_predictor(\n",
    "#     PredictorName=f\"{project}_cnnqr_rts\",\n",
    "#     AlgorithmArn=cnnqr_algorithm_arn,\n",
    "#     ForecastHorizon=forecast_horizon,\n",
    "#     PerformAutoML=False,\n",
    "#     PerformHPO=False,\n",
    "#     EvaluationParameters=evaluation_parameters,\n",
    "#     InputDataConfig=input_data_config,\n",
    "#     FeaturizationConfig=featurization_config,\n",
    "# )\n",
    "# results[\"CNN-QR with RTS\"] = SimpleNamespace(predictor_arn=cnnqr_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we'll need to **wait** for our predictors to finish training, to compare results. The below will poll to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_predictors = [results[r].predictor_arn for r in results]\n",
    "failed_predictors = []\n",
    "\n",
    "def check_status():\n",
    "    \"\"\"Check and update in_progress_predictors\"\"\"\n",
    "    just_stopped = []  # Can't edit the in_progress list directly the loop!\n",
    "    for arn in in_progress_predictors:\n",
    "        predictor_desc = forecast.describe_predictor(PredictorArn=arn)\n",
    "        status = predictor_desc[\"Status\"]\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f\"\\nBuild succeeded for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "        elif \"FAILED\" in status:\n",
    "            print(f\"\\nBuild failed for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "            failed_predictors.append(arn)\n",
    "    for arn in just_stopped:\n",
    "        in_progress_predictors.remove(arn)\n",
    "    return in_progress_predictors\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=check_status,\n",
    "    fn_is_finished=lambda l: len(l) == 0,\n",
    "    fn_stringify_result=lambda l: f\"{len(l)} predictor builds in progress\",\n",
    "    poll_secs=60,  # Poll every minute\n",
    "    timeout_secs=3*60*60,  # Max 3 hours\n",
    ")\n",
    "\n",
    "if len(failed_predictors):\n",
    "    raise RuntimeError(f\"The following predictors failed to train:\\n{failed_predictors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⏰ Predictor training can **take some time**: Simpler algorithms like ARIMA or ETS will typically train faster (may be ready in ~20mins on this example dataset), whereas more complex algorithms like DeepAR+ will usually take longer (may be approx 1hr on this example dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Predictors\n",
    "\n",
    "Once each of the Predictors is in an `Active` state, we can get metrics about it to better understand its accuracy and behavior. These are computed based on the hold out periods we defined when building the Predictor. The metrics are meant to guide our decisions when we use a particular Predictor to generate a forecast.\n",
    "\n",
    "As before, we'll define a utility function below to retriee the raw accuracy metrics response, and also build up our leaderboard. In the following cells, we'll run the function against each trained predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trial_metrics(trial_name=None) -> pd.DataFrame:\n",
    "    \"\"\"Utility to fetch the accuracy metrics for a predictor and output the leaderboard so far\"\"\"\n",
    "    if (trial_name):\n",
    "        # Print the raw API response:\n",
    "        metrics_response = forecast.get_accuracy_metrics(PredictorArn=results[trial_name].predictor_arn)\n",
    "        print(f\"Raw metrics for {trial_name}:\")\n",
    "        prettyprint(metrics_response)\n",
    "\n",
    "        # Save the payload section to results:\n",
    "        evaluation_results = metrics_response[\"PredictorEvaluationResults\"]\n",
    "        results[trial_name].evaluation_results = evaluation_results\n",
    "\n",
    "        # Construct simplified version for our comparison:\n",
    "        try:\n",
    "            summary_metrics = next(\n",
    "                w for w in evaluation_results[0][\"TestWindows\"] if w[\"EvaluationType\"] == \"SUMMARY\"\n",
    "            )[\"Metrics\"]\n",
    "        except StopIteration:\n",
    "            raise ValueError(\"Couldn't find SUMMARY metrics in Forecast API response\")\n",
    "        results[trial_name].summary_metrics = {\n",
    "            \"RMSE\": summary_metrics[\"RMSE\"],\n",
    "            \"10% wQL\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.1\n",
    "            ),\n",
    "            \"50% wQL (MAPE)\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.5\n",
    "            ),\n",
    "            \"90% wQL\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.9\n",
    "            ),\n",
    "        }\n",
    "    # Render the leaderboard:\n",
    "    return pd.DataFrame([\n",
    "        { \"Predictor\": name, **results[name].summary_metrics } for name in results\n",
    "        if \"summary_metrics\" in results[name].__dict__\n",
    "    ]).set_index(\"Predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet\n",
    "\n",
    "Let's compare the metrics for Prophet with RTS added to the previous predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trial_metrics(\"Prophet with RTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our test (your results may vary) the MAPE/wQL0.5; RMSE; and wQL0.9 scores were all slightly improved for the Prophet predictor by incorporating the RTS data. However, its performance still appeared generally worse than the DeepAR+ algorithm fitted on TTS data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+\n",
    "\n",
    "Let's add the metrics from our new RTS-aware DeepAR+ model into the view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trial_metrics(\"DeepAR+ with RTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our test, all quantile losses improved slightly for the DeepAR+ model when RTS data was added - although the RMSE did increase a little.\n",
    "\n",
    "In general, we could conclude that the models were improved by the addition of weather data but there could still be further work to do in finding additional significant factors that contribute to traffic volume, or possibly consolidating/combining the weather features to extract clearer signals for forecasting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_trial_metrics(\"CNN-QR with RTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Done!\n",
    "\n",
    "In this notebook, we updated our dataset group with a *Related Time-Series* dataset of additional (weather) data to try and improve the forecast from the initial baseline using the traffic volume history alone.\n",
    "\n",
    "You can refer to the previous notebooks 2a and 2b for guidance on visualizing the forecasts in the console and exporting + downloading them to compare against actual validation data.\n",
    "\n",
    "Identifying important related data such as stock availability, holiday & promotion calendars, pricing and similar can have dramatic impacts on real-world forecasting use cases; but it's just as important to understand any gaps, errors, or aggregations in your datasets to check that your models are interpreting your data as you expect.\n",
    "\n",
    "Check out the [Cleanup notebook](Cleanup.ipynb) for guidance on cleaning up your Amazon Forecast environment and also your Amazon S3 and AWS IAM setup from these experiments.\n",
    "\n",
    "If you've prepared your own data for import to Amazon Forecast, you might also be interested in the [Data Diagnostic notebook](Data%20Diagnostic.ipynb) which can help run some basic checks and graphs on your data."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
